---
title: "Car Prediction"
output: html_document
date: "2025-05-01"
---

```{r}
library(ggplot2)
library(car)
library(knitr)
library(MASS)
par(mfrow=c(2,2))
```
## Why?
Our goal is to predict the price of a used car and to understand what factors influence the price of the car.\
Americans on average spend around $600 every month on a car payement which is around 15% of their take home pay. So buying a car that is actually worth it is really important. So we want to make sure that we don't get scammed or overpay for a car.\

## Research Questions
1. What Car brand when new is the most expensive when all other features are the same?\
2. How close is our prediction to the actual value of a car?\
3. Does a specific car brand hold its value better?

## Data Loading and Exploration
We got our data from Kaggle.\
```{r}
# Load the dataset
car_data <- read.csv("BMGT430/car_price_prediction.csv")

# Vector of manufacturers to remove
remove_manufacturers <- c(
  "ZAZ", "სხვა", "PEUGEOT", "PONTIAC", "SCION", "SKODA", "SSANGYONG", "UAZ", 
  "GAZ", "DAIHATSU", "DAEWOO", "CITROEN", "GREATWALL", "HAVAL", "ISUZU", 
  "LANCIA", "MERCURY", "MOSKVICH", "OPEL", "RENAULT", "SAAB", "SATURN", "SEAT", "VAZ", "FERRARI", "ASTON MARTIN", "TESLA"
)

# Remove rows where Manufacturer is in the removal list
car_data <- car_data[!car_data$Manufacturer %in% remove_manufacturers, ]

# Check if first row contains column names and remove if needed
if(car_data[1,1] == "ID") {
  car_data <- car_data[-1,]
}

# Display dataset dimensions
dim(car_data)

# Display the first few rows
head(car_data)

# Check the structure
str(car_data)

#summary
summary(car_data)
```

## Exploratory Data Analysis

### Data Cleaning and Preparation\
We need to clean the data up a little\
First we need to make sure that the numeric data is represented as numerical data and remove any words that might be in the entries.
```{r}
# Convert character columns to appropriate types
car_data$Price <- as.numeric(car_data$Price)
car_data$Prod..year <- as.numeric(car_data$Prod..year)

# Clean Engine volume (remove "Turbo" from strings and convert to numeric)
car_data$Engine.volume <- as.numeric(gsub(" Turbo", "", car_data$Engine.volume))

# Clean Mileage (remove "km" and convert to numeric)
car_data$Mileage <- as.numeric(gsub(" km", "", car_data$Mileage))

car_data$Airbags <- as.numeric(car_data$Airbags)
```
\
Then we need to remove any rows that have any missing data and this is not a problem since we have so much data we can just get rid of the rows instead of having to fill in those missing points.\
```{r}
# Remove rows with missing values or impute them
car_data_clean <- na.omit(car_data)
```
The dataset we selected has a few cars that are way to expensive so lets get rid of those rows.\
Most of the cars under \$6,000 have issues that make them really cheap so we removed those cars as well.\
Cars that are more than \$100,000 are not realistic for us. Since we won't really be looking to buy cars that are that expensive.\
```{r}

car_data_clean <- car_data_clean[car_data_clean$Price >= 5000, ]
car_data_clean <- car_data_clean[car_data_clean$Price <= 100000, ]

attach(car_data_clean)
```
\
Age of the car is better than production year\
```{r}
car_data_clean$Age <- 2025 - car_data_clean$Prod..year

# Check dimensions after outlier removal
dim(car_data_clean)
```
\
We are in the US so we converted from km to miles to make it easier for us to interpret
```{r}
#Converting Mileage units from km to mi
car_data_clean$Mileage <- car_data_clean$Mileage * 0.621371
#Rounding to the nearest 2nd number
car_data_clean$Mileage <- round(car_data_clean$Mileage, 2)

attach(car_data_clean)
```
\
## Building Our Model
Here we have our first model which includes most of our columns. We got rid of Cylinders and Levy since most of the cars did not have this information. We decided not to look at the Model of the Car since there are too many and the model will become way to complex.
```{r}
base <- lm(Price ~ Age + Mileage + Engine.volume + Manufacturer + Category + Leather.interior + Fuel.type + Gear.box.type + Drive.wheels + Wheel + Color + Airbags, data = car_data_clean)
summary(base)
plot(base)
residuals <- residuals(base)
plot(1:length(residuals), residuals, main = "Residuals vs. Order")
```
\
Many of the assumptions are violated like Linearity the mean in the residuals vs fitted graph is not always 0.\
Equal variance is violated as there is a cone shape.\
Normality is also violated as there are points that deviate from the 45 degree line in the QQ plot.\
Lets try to take the log of Price to fix the residuals vs fitted graph since the distribution of Price is not normal.\
```{r}
hist(Price)
base2 <- lm(log(Price) ~ Age + Mileage + Engine.volume + Manufacturer + Category + Leather.interior + Fuel.type + Gear.box.type + Drive.wheels + Wheel + Color + Airbags, data = car_data_clean)
summary(base2)
plot(base2)
residuals <- residuals(base2)
plot(1:length(residuals), residuals, main = "Residuals vs. Order")
```
\
Some of the variables have a high p-value in the summary let us do a F-test to see if we can drop any of them.\
```{r}
drop1(base2, test="F")
```
\
We can get rid of Mileage and Color
```{r}
model <- lm(log(Price) ~ Age + Engine.volume + Manufacturer + Category + Leather.interior + Fuel.type + Gear.box.type + Drive.wheels + Wheel + Airbags, data = car_data_clean)
summary(model)
plot(model)
residuals <- residuals(model)
plot(1:length(residuals), residuals, main = "Residuals vs. Order")
```
Some of the dummy variables have high p-values let's try to see if we can get rid of some of them.\
```{r}
other_category <- c("Cabriolet", "Jeep", "Microbus")
car_data_clean$Category2 <- as.character(car_data_clean$Category)
car_data_clean$Category2[car_data_clean$Category2 %in% other_category] <- "Other"
car_data_clean$Category2 <- as.factor(car_data_clean$Category2)
car_data_clean$Category2 <- relevel(car_data_clean$Category2, ref = "Other")

# Re-fit the model
model2 <- lm(log(Price) ~ Age + Engine.volume +
             Manufacturer + Category2 + Leather.interior +
             Fuel.type + Gear.box.type + Drive.wheels +
             Wheel + Color + Airbags,
             data = car_data_clean)
anova(model2, model)

other_Fuel.type <- c("CNG", "Hydrogen", "Hybrid")
car_data_clean$Fuel.type2 <- as.character(car_data_clean$Fuel.type)
car_data_clean$Fuel.type2[car_data_clean$Fuel.type2 %in% other_Fuel.type] <- "Other"
car_data_clean$Fuel.type2 <- as.factor(car_data_clean$Fuel.type2)
car_data_clean$Fuel.type2 <- relevel(car_data_clean$Fuel.type2, ref = "Other")

# Re-fit the model
model3 <- lm(log(Price) ~ Age + Engine.volume +
             Manufacturer + Category2 + Leather.interior +
             Fuel.type2 + Gear.box.type + Drive.wheels +
             Wheel +Airbags,
             data = car_data_clean)
anova(model3, model2)
summary(model3)

# List of manufacturers to combine
other_makes <- c(
  "ACURA", "BUICK", "DODGE", "FIAT", "SUBARU", "CHRYSLER", "GMC", "VOLVO", "CHRYSLER", "CADILLAC"
)

# Combine into "Other"
car_data_clean$Manufacturer2 <- as.character(car_data_clean$Manufacturer)
car_data_clean$Manufacturer2[car_data_clean$Manufacturer2 %in% other_makes] <- "Other"
car_data_clean$Manufacturer2 <- as.factor(car_data_clean$Manufacturer2)
car_data_clean$Manufacturer2 <- relevel(car_data_clean$Manufacturer2, ref = "Other")

# Re-fit the model
model4 <- lm(log(Price) ~ Age + Engine.volume +
             Manufacturer2 + Category2 + Leather.interior +
             Fuel.type2 + Gear.box.type + Drive.wheels +
             Wheel + Airbags,
             data = car_data_clean)
anova(model4, model3)
summary(model4)

other_drive <- c("4x4","Front")
# Combine into "Other"
car_data_clean$Drive.wheels2 <- as.character(car_data_clean$Drive.wheels)
car_data_clean$Drive.wheels2[car_data_clean$Drive.wheels2 %in% other_drive] <- "Other"
car_data_clean$Drive.wheels2 <- as.factor(car_data_clean$Drive.wheels2)
car_data_clean$Drive.wheels2 <- relevel(car_data_clean$Drive.wheels2, ref = "Other")

# Re-fit the model
model5 <- lm(log(Price) ~ Age + Engine.volume +
             Manufacturer2 + Category2 + Leather.interior +
             Fuel.type2 + Gear.box.type + Drive.wheels2 +
             Wheel + Airbags,
             data = car_data_clean)
anova(model5, model4)

summary(model5)
plot(model5)
residuals <- residuals(model5)
plot(1:length(residuals), residuals, main = "Residuals vs. Order")
```
\
The log transformation improved the linearity assumption but the the other assumptions are still violated.\
Let's try Box-Cox transformation\
```{r}
bc <- boxcox(model5, lambda = seq(-2, 2, 0.1))
best_lambda <- bc$x[which.max(bc$y)]
# Transform the response variable
if (abs(best_lambda) < 1e-6) {
  car_data_clean$Price_transformed <- log(car_data_clean$Price)
} else {
  car_data_clean$Price_transformed <- (car_data_clean$Price^best_lambda - 1) / best_lambda
}
base_transformed <- lm(Price_transformed ~ Age + Engine.volume + Manufacturer2 + Category2 + Leather.interior + Fuel.type2 + Gear.box.type + Drive.wheels + Wheel + Airbags, data = car_data_clean)

summary(base_transformed)
plot(base_transformed)
attach(car_data_clean)
```
\
Still have the same issue and not too much better than log so lets stick with log since it is easier to interpret.\
\
I am going to remove all of the outliers which is all of the points where the absolute value of the standardized residuals is greater than 2 and any high leverage points.
```{r}

std_res <- rstandard(model5)

outliers <- abs(std_res) > 2

final_data <- car_data_clean[!outliers, ]

leverage <- hatvalues(model5)

# Define threshold for high leverage points
p <- length(coef(model5))  # number of model parameters
n <- nrow(final_data)  # number of observations
threshold <- 3 * p / n     # common rule-of-thumb threshold

# Identify high leverage points
high_leverage_points <- leverage > threshold

# Remove high leverage points from the data
final_data <- final_data[!high_leverage_points, ]
```
\
## Final Model
```{r}
model5 <- lm(log(Price) ~ Age + Engine.volume +
             Manufacturer2 + Category2 + Leather.interior +
             Fuel.type2 + Gear.box.type + Drive.wheels2 +
             Wheel + Airbags,
             data = final_data)
summary(model5)
plot(model5)
residuals <- residuals(model5)
plot(1:length(residuals), residuals, main = "Residuals vs. Order")
```
\
With the new model we have fixed linearity assumption as the mean is almost entirely 0 in the residuals vs fitted plot. We tried to use log and box cox transformations to fix the normality and equal spread issue. The normality is better and so is teh equal spread but it is still an issue. There seems to be independence since the residuals vs Order graph does not have any pattern.
How do we interpret the new model:\

## Research Questions
### What Car brand when new is the most expensive and all other features are the same?
Jaguar has the highest coefficents out of all of the Manufacturers: 0.9397321\
100*(e^(0.9397321)-1) = 155.9296%. So a Jaguar car is 155.9296% higher in Price on average in comparison to "base manufacturers" for a new car (Age=0) while taking into account other variables.
The "base manufacturers" are ACURA, BUICK, DODGE, FIAT, SUBARU, CHRYSLER, GMC, VOLVO, CHRYSLER, CADILLAC

### Predicting Car Prices
Let us try predicting the price of this car which is being sold for $18,992\
https://www.capitalone.com/cars/vehicle-details/2023/Nissan/Sentra/SR/3N1AB8DV3PY240617\
```{r}
newcar <- data.frame(
  Age = 2,
  Mileage = 36,454,
  Engine.volume = 2.0,
  Manufacturer2 = "Other",
  Category2 = "Sedan",
  Leather.interior = "No",
  Fuel.type2 = "Petrol",
  Gear.box.type = "Automatic",
  Drive.wheels2 = "Other",
  Wheel = "Left wheel",
  Color2 = "Other", #White
  Airbags = 8
)

# Predict transformed price
pred <- predict(model5, newdata = newcar, interval = "prediction", level = 0.95)
# pred is a matrix with columns: fit (mean), lwr (lower CI), upr (upper CI)

price_pred <- exp(pred)

print(price_pred)
```
The actual price is between our confidence interval and only around \$1000 off our point estimate.\
The model does a good job predicting the prices of cars.\
Our model could be useful when it come to making a decision on whether to buy a car or not as it will help people understand if a car listing is too high.\
Here are predictions to other cars\
```{r}
### 2011 Silver Toyota Sedan with 100,000 miles, 2.4L engine, leather interior, takes gasoline as fuel, automatic transmission, front wheel drive, driver side on the left, and 4 airbags. A standard 2011 Toyota Sedan.
obs <- data.frame(Age = 14, Mileage = 100000, Engine.volume = 2.4, Manufacturer2 = "TOYOTA", Category2 = "Sedan", Leather.interior = "Yes", Fuel.type2 = "Petrol", Gear.box.type = "Automatic", Drive.wheels2 = "Other", Wheel = "Left wheel", Color2 = "Silver", Airbags = 4)

pred2 <- predict(model5, obs, interval = "prediction")

#Transforming final price back to dollars from log
price_pred2 <- exp(pred2)

print(price_pred2)

#-------------------------------------------------------------------------------------------------------------

### 2020 Silver Mercedes-Benz Sedan with 40,000 miles, 3.5L engine, leather interior, takes gasoline as fuel, automatic transmission, front wheel drive, driver side on the left, and 4 airbags. A more expensive vehicle.

obs_2 <- data.frame(Age = 5, Mileage = 40000, Engine.volume = 3.5, Manufacturer2 = "MERCEDES-BENZ", Category2 = "Sedan", Leather.interior = "Yes", Fuel.type2 = "Petrol", Gear.box.type = "Automatic", Drive.wheels2 = "Other", Wheel = "Left wheel", Color2 = "Silver", Airbags = 4)


pred_3 <- predict(model5, obs_2, interval = "prediction")

#Transforming final price back to dollars from log

#Transforming final price back to dollars from box-cox
price_pred3 <- exp(pred_3)

print(price_pred3)


#------------------------------------------------------------------------------------------------------------
### 2017 Silver Honda Sedan with 60,000 miles, 2.4L engine, leather interior, takes gasoline as fuel, automatic transmission, front wheel drive, driver side on the left, and 4 airbags. Falls in between first and second predicted vehicle prices as expected.

obs_3 <- data.frame(Age = 8, Mileage = 60000, Engine.volume = 2.4, Manufacturer2 = "HONDA", Category2 = "Sedan", Leather.interior = "Yes", Fuel.type2 = "Petrol", Gear.box.type = "Automatic", Drive.wheels2 = "Other", Wheel = "Left wheel", Color2 = "Silver", Airbags = 4)


pred_4 <- predict(model5, obs_3, interval = "prediction")

#Transforming final price back to dollars from log
price_pred4 <- exp(pred_4)

print(price_pred4)
```

We looked online and many of the cars with similar features and stats are a little bit lower in price, but they are in the confidence interval.\
The higher prediction could be because out dataset is 3 years old when car prices were a little higher.\

