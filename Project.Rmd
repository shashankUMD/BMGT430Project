---
title: "project"
output: html_document
date: "2025-05-01"
---

```{r}
library(ggplot2)
library(car)
library(knitr)
library(MASS)
```
#Car Price Prediction
Our goal is to predict the price of a used car and to understand what factors influence the price of the car.\
Americans on average spend around $600 every month on a car payement which is around 15% of their take home pay. So buying a car that is actually worth it is really important. So we want to make sure that we don't get scammed or overpay for a car.\
###Research Questions
1. What is the major variable that determines the price of a car?\
2. How close is our prediction to the actual value of a car?\
3. Does a specific car brand hold its value better?

###Data Loading and Exploration
```{r}
# Load the dataset
car_data <- read.csv("car_price_prediction.csv")

# Check if first row contains column names and remove if needed
if(car_data[1,1] == "ID") {
  car_data <- car_data[-1,]
}

# Display dataset dimensions
dim(car_data)

# Display the first few rows
head(car_data)

# Check the structure
str(car_data)
```

##Exploratory Data Analysis

###Data Cleaning and Preparation\
We need to clean the data up a little\
First we need to make sure that the numeric data is represented as numerical data and remove any words that might be in the entries.
```{r}
# Convert character columns to appropriate types
car_data$Price <- as.numeric(car_data$Price)
car_data$Levy <- as.numeric(ifelse(car_data$Levy == "-", NA, car_data$Levy))
car_data$Prod..year <- as.numeric(car_data$Prod..year)

# Clean Engine volume (remove "Turbo" from strings and convert to numeric)
car_data$Engine.volume <- as.numeric(gsub(" Turbo", "", car_data$Engine.volume))

# Clean Mileage (remove "km" and convert to numeric)
car_data$Mileage <- as.numeric(gsub(" km", "", car_data$Mileage))

car_data$Cylinders <- as.numeric(car_data$Cylinders)
car_data$Airbags <- as.numeric(car_data$Airbags)
```
\
Then we need to remove any rows that have any missing data and this is not a problem since we have so much data we can just get rid of the rows instead of having to fill in those missing points.\
```{r}
# Remove rows with missing values or impute them
car_data_clean <- na.omit(car_data)
```
The dataset we selected has a few cars that are way to expensive so lets get rid of those rows.\
Most of the cars under \$10,000 have issues that make them really cheap so we removed those cars as well.\
```{r}

car_data_clean <- car_data_clean[car_data_clean$Price >= 5000, ]
car_data_clean <- car_data_clean[car_data_clean$Price <= 100000, ]

attach(car_data_clean)
```
\
Age of the car is better than production year\
```{r}
car_data_clean$Age <- 2025 - car_data_clean$Prod..year

# Check dimensions after outlier removal
dim(car_data_clean)
```
\
We are in the US so we converted from km to miles to make it easier for us to interpret
```{r}
#Converting Mileage units from km to mi
car_data_clean$Mileage <- car_data_clean$Mileage * 0.621371
#Rounding to the nearest 2nd number
car_data_clean$Mileage <- round(car_data_clean$Mileage, 2)

attach(car_data_clean)
```
\
##Building Our Model
Here we have our first model which includes most of our columns. We got rid of Cylinders and Levy since most of the cars did not have this information. We decided not to look at the Model of the Car since there are too many and the model will become way to complex.
```{r}
base <- lm(Price ~ Age + Mileage + Engine.volume + factor(Manufacturer) + factor(Category) + factor(Leather.interior) + factor(Fuel.type) + factor(Gear.box.type) + factor(Drive.wheels) + factor(Wheel) + factor(Color) + Airbags, data = car_data_clean)
summary(base)
plot(base)
```
\
Many of the assumptions are violated like Linearity the mean in the residuals vs fitted graph is not always 0.\
Equal variance is violated as there is a cone shape.\
Normality is also violated as there are points that deviate from the 45 degree line in the QQ plot.\
Lets try to take the log of Price to fix the residuals vs fitted graph since the distribution of Price is not normal.\
```{r}
hist(Price)
model <- lm(log(Price) ~ Age + Mileage + Engine.volume + factor(Manufacturer) + factor(Category) + factor(Leather.interior) + factor(Fuel.type) + factor(Gear.box.type) + factor(Drive.wheels) + factor(Wheel) + factor(Color) + Airbags, data = car_data_clean)
summary(model)
plot(model)
```
\
The log transformation improved the linearity assumption but the the other assumptions are still violated.\
Let's try Box-Cox transformation\
```{r}
bc <- boxcox(base, lambda = seq(-2, 2, 0.1))
best_lambda <- bc$x[which.max(bc$y)]
# Transform the response variable
if (abs(best_lambda) < 1e-6) {
  car_data_clean$Price_transformed <- log(car_data_clean$Price)
} else {
  car_data_clean$Price_transformed <- (car_data_clean$Price^best_lambda - 1) / best_lambda
}
base_transformed <- lm(Price_transformed ~ Age + Mileage + Engine.volume + factor(Manufacturer) +
                       factor(Category) + factor(Leather.interior) + factor(Fuel.type) +
                       factor(Gear.box.type) + factor(Drive.wheels) + factor(Wheel) +
                       factor(Color) + Airbags, data = car_data_clean)
summary(base_transformed)
plot(base_transformed)
attach(car_data_clean)
```
\
Still have the same issue but much better.\

We thought that different car brands might hold their value differently as they become older so we added the interaction term Manufacturer*Age. So lets get the Age vs Price plot for a few different manufacturers\
```{r}
 # Filter data
honda_data <- subset(car_data_clean, Manufacturer == "HONDA")

# Plot
plot(honda_data$Age, honda_data$Price,
    main = "Age vs Price for Honda Cars",
    xlab = "Age (Years)", ylab = "Price",
    col = "blue", pch = 19)
 
 # Filter data
hyundai_data <- subset(car_data_clean, Manufacturer == "HYUNDAI")

# Plot
plot(hyundai_data$Age, hyundai_data$Price,
     main = "Age vs Price for Hyundai Cars",
     xlab = "Age (Years)", ylab = "Price",
     col = "blue", pch = 19)

# Filter data
VOLKSWAGEN_data <- subset(car_data_clean, Manufacturer == "VOLKSWAGEN")

# Plot
plot(VOLKSWAGEN_data$Age, VOLKSWAGEN_data$Price,
     main = "Age vs Price for VOLKSWAGEN Cars",
     xlab = "Age (Years)", ylab = "Price",
     col = "blue", pch = 19)

# Filter data
FORD_data <- subset(car_data_clean, Manufacturer == "FORD")

# Plot
plot(FORD_data$Age, FORD_data$Price,
     main = "Age vs Price for FORD Cars",
     xlab = "Age (Years)", ylab = "Price",
     col = "blue", pch = 19)
```
\
There seems to be some differences in the slopes so lets add the interaction term.
```{r}
model2<-lm(Price_transformed ~ Age + Mileage + Engine.volume + factor(Manufacturer) + Manufacturer:Age + factor(Category) + factor(Leather.interior) + factor(Fuel.type) + factor(Gear.box.type) + factor(Drive.wheels) + factor(Wheel) + factor(Color) + Airbags, data = car_data_clean)
summary(model2)
plot(model2)
```
\
Maybe there is a relationship between different Manufacturers and the mileage
```{r}
plot(FORD_data$Mileage, FORD_data$Price,
     main = "Mileage vs Price for FORD Cars",
     xlab = "Mileage (Miles)", ylab = "Price",
     col = "blue", pch = 19)
plot(VOLKSWAGEN_data$Mileage, VOLKSWAGEN_data$Price,
     main = "Mileage vs Price for VOLKSWAGEN Cars",
     xlab = "Mileage (Miles)", ylab = "Price",
     col = "blue", pch = 19)
plot(hyundai_data$Mileage, hyundai_data$Price,
     main = "Mileage vs Price for Hyundai Cars",
     xlab = "Mileage (Miles)", ylab = "Price",
     col = "blue", pch = 19)
plot(honda_data$Mileage, honda_data$Price,
     main = "Mileage vs Price for Honda Cars",
     xlab = "Mileage (Miles)", ylab = "Price",
     col = "blue", pch = 19)
```
\
There seems to a different slope for the different brands
```{r}
model3<-lm(Price_transformed ~ Age + Mileage + Engine.volume + factor(Manufacturer) + Manufacturer:Age + Manufacturer:Mileage + factor(Category) + factor(Leather.interior) + factor(Fuel.type) + factor(Gear.box.type) + factor(Drive.wheels) + factor(Wheel) + factor(Color) + Airbags, data = car_data_clean)
summary(model3)
plot(model3)
```
\
I am going to remove all of the outliers which is all of the points where the absolute value of the standardized residuals is greater than 2
```{r}

std_res <- rstandard(model3)

outliers <- abs(std_res) > 2

final_data <- car_data_clean[!outliers, ]

final_data <- final_data[!final_data$Manufacturer %in% c("FERRARI", "MERCURY", "SCION", "CADILLAC"), ]
```
\
##Final Model
```{r}
model3<-lm(Price_transformed ~ Age + Mileage + Engine.volume + factor(Manufacturer) + Manufacturer:Age + Manufacturer:Mileage + factor(Category) + factor(Leather.interior) + factor(Fuel.type) + factor(Gear.box.type) + factor(Drive.wheels) + factor(Wheel) + factor(Color) + Airbags, data = final_data)
summary(model3)
plot(model3)
residuals <- residuals(model3)
plot(1:length(residuals), residuals, main = "Residuals vs. Order")
```
\
With the new model we have fixed linearity assumption as the mean is almost entirely 0 in the residuals vs fitted plot. We tried to use log and box cox transformations to fix the normality and equal spread issue. The normality is better and so is the equal spread but it is still an issue. There seems to be independence since the residuals vs Order graph does not have any pattern.
How do we interpret the new model:\

##Predicting Prices
```{r}
### 2011 Silver Toyota Sedan with 100,000 miles, 2.4L engine, leather interior, takes gasoline as fuel, automatic transmission, front wheel drive, driver side on the left, and 4 airbags. A standard 2011 Toyota Sedan.
obs <- data.frame(Age = 14, Mileage = 100000, Engine.volume = 2.4, Manufacturer = "TOYOTA", Category = "Sedan", Leather.interior = "Yes", Fuel.type = "Petrol", Gear.box.type = "Automatic", Drive.wheels = "Front", Wheel = "Left wheel", Color = "Silver", Airbags = 4)

predict(model3, obs, interval = "confidence")

pred <- predict(model3, obs, interval = "confidence")

#Transforming final price back to dollars from log
pred_back <- exp(pred)

#Transforming final price back to dollars from box-cox
lambda_price <- best_lambda
pred_back <- (lambda_price * pred + 1)^(1 / lambda_price)
pred_back

#-------------------------------------------------------------------------------------------------------------

### 2020 Silver Mercedes-Benz Sedan with 40,000 miles, 3.5L engine, leather interior, takes gasoline as fuel, automatic transmission, front wheel drive, driver side on the left, and 4 airbags. A more expensive vehicle.

obs_2 <- data.frame(Age = 5, Mileage = 40000, Engine.volume = 3.5, Manufacturer = "MERCEDES-BENZ", Category = "Sedan", Leather.interior = "Yes", Fuel.type = "Petrol", Gear.box.type = "Automatic", Drive.wheels = "Front", Wheel = "Left wheel", Color = "Silver", Airbags = 4)

predict(model3, obs_2, interval = "confidence")

pred_2 <- predict(model3, obs_2, interval = "confidence")

#Transforming final price back to dollars from log
pred_back_2 <- exp(pred_2)

#Transforming final price back to dollars from box-cox
lambda_price <- best_lambda
pred_back_2 <- (lambda_price * pred_2 + 1)^(1 / lambda_price)
pred_back_2

#-------------------------------------------------------------------------------------------------------------

### 2017 Silver Honda Sedan with 60,000 miles, 2.4L engine, leather interior, takes gasoline as fuel, automatic transmission, front wheel drive, driver side on the left, and 4 airbags. Falls in between first and second predicted vehicle prices as expected.

obs_3 <- data.frame(Age = 8, Mileage = 60000, Engine.volume = 2.4, Manufacturer = "HONDA", Category = "Sedan", Leather.interior = "Yes", Fuel.type = "Petrol", Gear.box.type = "Automatic", Drive.wheels = "Front", Wheel = "Left wheel", Color = "Silver", Airbags = 4)

predict(model3, obs_3, interval = "confidence")

pred_3 <- predict(model3, obs_3, interval = "confidence")

#Transforming final price back to dollars from log
pred_back_3 <- exp(pred_3)

#Transforming final price back to dollars from box-cox
lambda_price <- best_lambda
pred_back_3 <- (lambda_price * pred_3 + 1)^(1 / lambda_price)
pred_back_3
```

